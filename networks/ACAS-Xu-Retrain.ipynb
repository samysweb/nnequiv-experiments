{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title: Knowledge Distillation\n",
    "Author: [Kenneth Borup](https://twitter.com/Kennethborup)\n",
    "Date created: 2020/09/01\n",
    "Last modified: 2020/09/01\n",
    "Description: Implementation of classical Knowledge Distillation.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## Introduction to Knowledge Distillation\n",
    "Knowledge Distillation is a procedure for model\n",
    "compression, in which a small (student) model is trained to match a large pre-trained\n",
    "(teacher) model. Knowledge is transferred from the teacher model to the student\n",
    "by minimizing a loss function, aimed at matching softened teacher logits as well as\n",
    "ground-truth labels.\n",
    "The logits are softened by applying a \"temperature\" scaling function in the softmax,\n",
    "effectively smoothing out the probability distribution and revealing\n",
    "inter-class relationships learned by the teacher.\n",
    "**Reference:**\n",
    "- [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## Setup\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = onnxruntime.InferenceSession(\"../benchmarks/ACASXU_run2a_1_2_batch_2000-trunc.onnx\")\n",
    "inname = [input.name for input in session.get_inputs()]\n",
    "outname = [output.name for output in session.get_outputs()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from [this tutorial](https://github.com/keras-team/keras-io/blob/master/examples/vision/knowledge_distillation.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6030121 , -0.16638894,  0.27088654,  0.45687816, -0.48346344],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u, l = ([0.679858, 0.500000, 0.500000, 0.500000, -0.450000 ],[0.600000, -0.500000, -0.500000, 0.450000, -0.500000])\n",
    "x_train = np.array(np.random.default_rng().uniform(l,u,(200000,5)),dtype=np.float32)\n",
    "x_train[0]\n",
    "x_train[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0202845, -0.0193085, -0.019157 , -0.0189128, -0.0189197]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = []\n",
    "for d in x_train:\n",
    "    y_train.append(session.run(outname, {inname[0]: d}))\n",
    "y_train = np.array(y_train)\n",
    "y_train[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train[:,0,:]\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_train[150000:]\n",
    "y_test = y_train[150000:]\n",
    "x_train = x_train[:150000]\n",
    "y_train = y_train[:150000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 5)\n",
      "(150000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Create student and teacher models\n",
    "Initialy, we create a teacher model and a smaller student model. Both models are\n",
    "convolutional neural networks and created using `Sequential()`,\n",
    "but could be any Keras model.\n",
    "\"\"\"\n",
    "\n",
    "# Create the student\n",
    "student = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(5,)),\n",
    "        layers.Dense(50, activation='relu'),\n",
    "        layers.Dense(50, activation='relu'),\n",
    "        layers.Dense(50, activation='relu'),\n",
    "        layers.Dense(50, activation='relu'),\n",
    "        layers.Dense(50, activation='relu'),\n",
    "        layers.Dense(50, activation='relu'),\n",
    "        layers.Dense(5),\n",
    "    ],\n",
    "    name=\"student\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Construct `Distiller()` class\n",
    "The custom `Distiller()` class, overrides the `Model` methods `train_step`, `test_step`,\n",
    "and `compile()`. In order to use the distiller, we need:\n",
    "- A trained teacher model\n",
    "- A student model to train\n",
    "- A student loss function on the difference between student predictions and ground-truth\n",
    "- A distillation loss function, along with a `temperature`, on the difference between the\n",
    "soft student predictions and the soft teacher labels\n",
    "- An `alpha` factor to weight the student and distillation loss\n",
    "- An optimizer for the student and (optional) metrics to evaluate performance\n",
    "In the `train_step` method, we perform a forward pass of both the teacher and student,\n",
    "calculate the loss with weighting of the `student_loss` and `distillation_loss` by `alpha` and\n",
    "`1 - alpha`, respectively, and perform the backward pass. Note: only the student weights are updated,\n",
    "and therefore we only calculate the gradients for the student weights.\n",
    "In the `test_step` method, we evaluate the student model on the provided dataset.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher, inname, outname):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "        self.inname = inname\n",
    "        self.outname = outname\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics, run_eagerly=True)\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x,y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        #teacher_predictions = self.teacher(x, training=False)\n",
    "        #teacher_predictions = []\n",
    "        #for d in x:\n",
    "        #    #print(d)\n",
    "        #    #teacher_in = np.array(d,dtype=np.float32)\n",
    "        #    teacher_predictions.append(self.teacher.run(outname, {inname[0]: d.numpy()}))\n",
    "        #teacher_predictions = np.array(teacher_predictions)[:,0,:]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            #student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                y,\n",
    "                student_predictions,\n",
    "            )\n",
    "            loss = distillation_loss\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        #self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.distillation_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        #self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Distill teacher to student\n",
    "We have already trained the teacher model, and we only need to initialize a\n",
    "`Distiller(student, teacher)` instance, `compile()` it with the desired losses,\n",
    "hyperparameters and optimizer, and distill the teacher to the student.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize and compile distiller\n",
    "distiller = Distiller(student, session, inname, outname)\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[],\n",
    "    distillation_loss_fn=keras.losses.MeanSquaredError(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 18s 11ms/step - student_loss: 6.1918e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate student on test dataset\n",
    "distiller.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4688/4688 [==============================] - 75s 16ms/step - distillation_loss: 5.0567e-07\n",
      "Epoch 2/2\n",
      "4688/4688 [==============================] - 58s 12ms/step - distillation_loss: 7.8597e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9706aa8d90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def scheduler(epoch, lr):\n",
    "#    if epoch < 10:\n",
    "#        return lr\n",
    "#    else:\n",
    "#        return lr * tf.math.exp(-0.1)\n",
    "#callback = keras.callbacks.LearningRateScheduler(scheduler)\n",
    "# Distill teacher to student\n",
    "distiller.fit(x_train, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 7s 4ms/step - student_loss: 4.9775e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate student on test dataset\n",
    "distiller.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.save(\"ACASXU_run2a_1_2_batch_2000-retrain.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "import onnx\n",
    "import keras2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 25 -> 20\n",
      "The maximum opset needed by this model is only 9.\n"
     ]
    }
   ],
   "source": [
    "model = load_model('ACASXU_run2a_1_2_batch_2000-retrain.h5')\n",
    "onnx_model = keras2onnx.convert_keras(model, model.name)\n",
    "onnx.save_model(onnx_model, \"ACASXU_run2a_1_2_batch_2000-retrain.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.0202845, -0.0193085, -0.019157 , -0.0189128, -0.0189197],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(outname,{inname[0]:[0.679858, -0.03432750175414749, 0.385071, 0.5, -0.5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnequiv",
   "language": "python",
   "name": "nnequiv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
