{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from [this tutorial](https://github.com/keras-team/keras-io/blob/master/examples/vision/knowledge_distillation.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Title: Knowledge Distillation\n",
    "Author: [Kenneth Borup](https://twitter.com/Kennethborup)\n",
    "Date created: 2020/09/01\n",
    "Last modified: 2020/09/01\n",
    "Description: Implementation of classical Knowledge Distillation.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## Introduction to Knowledge Distillation\n",
    "Knowledge Distillation is a procedure for model\n",
    "compression, in which a small (student) model is trained to match a large pre-trained\n",
    "(teacher) model. Knowledge is transferred from the teacher model to the student\n",
    "by minimizing a loss function, aimed at matching softened teacher logits as well as\n",
    "ground-truth labels.\n",
    "The logits are softened by applying a \"temperature\" scaling function in the softmax,\n",
    "effectively smoothing out the probability distribution and revealing\n",
    "inter-class relationships learned by the teacher.\n",
    "**Reference:**\n",
    "- [Hinton et al. (2015)](https://arxiv.org/abs/1503.02531)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "## Setup\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Construct `Distiller()` class\n",
    "The custom `Distiller()` class, overrides the `Model` methods `train_step`, `test_step`,\n",
    "and `compile()`. In order to use the distiller, we need:\n",
    "- A trained teacher model\n",
    "- A student model to train\n",
    "- A student loss function on the difference between student predictions and ground-truth\n",
    "- A distillation loss function, along with a `temperature`, on the difference between the\n",
    "soft student predictions and the soft teacher labels\n",
    "- An `alpha` factor to weight the student and distillation loss\n",
    "- An optimizer for the student and (optional) metrics to evaluate performance\n",
    "In the `train_step` method, we perform a forward pass of both the teacher and student,\n",
    "calculate the loss with weighting of the `student_loss` and `distillation_loss` by `alpha` and\n",
    "`1 - alpha`, respectively, and perform the backward pass. Note: only the student weights are updated,\n",
    "and therefore we only calculate the gradients for the student weights.\n",
    "In the `test_step` method, we evaluate the student model on the provided dataset.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Distiller(keras.Model):\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1,\n",
    "        temperature=3,\n",
    "    ):\n",
    "        \"\"\" Configure the distiller.\n",
    "        Args:\n",
    "            optimizer: Keras optimizer for the student weights\n",
    "            metrics: Keras metrics for evaluation\n",
    "            student_loss_fn: Loss function of difference between student\n",
    "                predictions and ground-truth\n",
    "            distillation_loss_fn: Loss function of difference between soft\n",
    "                student predictions and soft teacher predictions\n",
    "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
    "            temperature: Temperature for softening probability distributions.\n",
    "                Larger temperature gives softer distributions.\n",
    "        \"\"\"\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack data\n",
    "        x, y = data\n",
    "\n",
    "        # Forward pass of teacher\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass of student\n",
    "            student_predictions = self.student(x, training=True)\n",
    "\n",
    "            # Compute losses\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss\n",
    "\n",
    "        # Compute gradients\n",
    "        trainable_vars = self.student.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "        # Update weights\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Update the metrics configured in `compile()`.\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update(\n",
    "            {\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "\n",
    "        # Compute predictions\n",
    "        y_prediction = self.student(x, training=False)\n",
    "\n",
    "        # Calculate the loss\n",
    "        student_loss = self.student_loss_fn(y, y_prediction)\n",
    "\n",
    "        # Update the metrics.\n",
    "        self.compiled_metrics.update_state(y, y_prediction)\n",
    "\n",
    "        # Return a dict of performance\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss})\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Prepare the dataset\n",
    "The dataset used for training the teacher and distilling the teacher is\n",
    "[MNIST](https://keras.io/api/datasets/mnist/), and the procedure would be equivalent for any other\n",
    "dataset, e.g. [CIFAR-10](https://keras.io/api/datasets/cifar10/), with a suitable choice\n",
    "of models. Both the student and teacher are trained on the training set and evaluated on\n",
    "the test set.\n",
    "\"\"\"\n",
    "\n",
    "# Prepare the train and test dataset.\n",
    "batch_size = 64\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def resize_img(data):\n",
    "    output = np.zeros((data.shape[0],64))\n",
    "    for i, img in enumerate(data):\n",
    "        output[i]=resize(img, (8,8)).flatten()\n",
    "    return output\n",
    "\n",
    "x_train_small = resize_img(x_train)\n",
    "y_train_small = resize_img(y_train)\n",
    "x_test_small = resize_img(x_test)\n",
    "y_test_small = resize_img(y_test)\n",
    "\n",
    "\n",
    "#bottle_resized = resize(bottle, (140, 54))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "x_train = x_train_small.astype(\"float32\") / 255.0\n",
    "x_train = np.reshape(x_train_small, (-1, 64))\n",
    "\n",
    "x_test = x_test_small.astype(\"float32\") / 255.0\n",
    "x_test = np.reshape(x_test_small, (-1, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Create student and teacher models\n",
    "Initialy, we create a teacher model and a smaller student model. Both models are\n",
    "convolutional neural networks and created using `Sequential()`,\n",
    "but could be any Keras model.\n",
    "\"\"\"\n",
    "\n",
    "# Create the teacher\n",
    "teacher = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(64,)),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(80, activation='relu'),\n",
    "        layers.Dense(60, activation='relu'),\n",
    "        layers.Dense(40, activation='relu'),\n",
    "        layers.Dense(20, activation='relu'),\n",
    "        layers.Dense(10),\n",
    "    ],\n",
    "    name=\"teacher\",\n",
    ")\n",
    "\n",
    "# Create the student\n",
    "student = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(64,)),\n",
    "        layers.Dense(40, activation='relu'),\n",
    "        layers.Dense(40, activation='relu'),\n",
    "        layers.Dense(10),\n",
    "    ],\n",
    "    name=\"student\",\n",
    ")\n",
    "\n",
    "# Clone student for later comparison\n",
    "student_scratch = keras.models.clone_model(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.4302 - sparse_categorical_accuracy: 0.8623\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1848 - sparse_categorical_accuracy: 0.9427\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1471 - sparse_categorical_accuracy: 0.9536\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1261 - sparse_categorical_accuracy: 0.9609\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1120 - sparse_categorical_accuracy: 0.9639\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1014 - sparse_categorical_accuracy: 0.9674\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0915 - sparse_categorical_accuracy: 0.9700\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0857 - sparse_categorical_accuracy: 0.9723\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0785 - sparse_categorical_accuracy: 0.9754\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0747 - sparse_categorical_accuracy: 0.9758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fca0bd72be0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Train the teacher\n",
    "In knowledge distillation we assume that the teacher is trained and fixed. Thus, we start\n",
    "by training the teacher model on the training set in the usual way.\n",
    "\"\"\"\n",
    "\n",
    "# Train teacher as usual\n",
    "teacher.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train and evaluate teacher on data.\n",
    "teacher.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0999 - sparse_categorical_accuracy: 0.9706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0999349057674408, 0.9706000089645386]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher.save(\"mnist8x8_100_80_60_40_20_10.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1875/1875 [==============================] - 3s 2ms/step - sparse_categorical_accuracy: 0.8329 - student_loss: 0.5669 - distillation_loss: 0.0675\n",
      "Epoch 2/30\n",
      "1875/1875 [==============================] - 3s 2ms/step - sparse_categorical_accuracy: 0.9074 - student_loss: 0.3001 - distillation_loss: 0.0323\n",
      "Epoch 3/30\n",
      "1875/1875 [==============================] - 3s 2ms/step - sparse_categorical_accuracy: 0.9276 - student_loss: 0.2331 - distillation_loss: 0.0229\n",
      "Epoch 4/30\n",
      "1875/1875 [==============================] - 3s 2ms/step - sparse_categorical_accuracy: 0.9389 - student_loss: 0.1936 - distillation_loss: 0.0176\n",
      "Epoch 5/30\n",
      "1875/1875 [==============================] - 3s 2ms/step - sparse_categorical_accuracy: 0.9463 - student_loss: 0.1710 - distillation_loss: 0.0147\n",
      "Epoch 6/30\n",
      "1875/1875 [==============================] - 3s 2ms/step - sparse_categorical_accuracy: 0.9517 - student_loss: 0.1554 - distillation_loss: 0.0129\n",
      "Epoch 7/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - sparse_categorical_accuracy: 0.9546 - student_loss: 0.1449 - distillation_loss: 0.0117\n",
      "Epoch 8/30\n",
      "1875/1875 [==============================] - 3s 2ms/step - sparse_categorical_accuracy: 0.9572 - student_loss: 0.1364 - distillation_loss: 0.0109\n",
      "Epoch 9/30\n",
      "1875/1875 [==============================] - 3s 2ms/step - sparse_categorical_accuracy: 0.9597 - student_loss: 0.1295 - distillation_loss: 0.0103\n",
      "Epoch 10/30\n",
      "1875/1875 [==============================] - 3s 2ms/step - sparse_categorical_accuracy: 0.9607 - student_loss: 0.1241 - distillation_loss: 0.0098\n",
      "Epoch 11/30\n",
      "1875/1875 [==============================] - 3s 2ms/step - sparse_categorical_accuracy: 0.9619 - student_loss: 0.1193 - distillation_loss: 0.0094\n",
      "Epoch 12/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - sparse_categorical_accuracy: 0.9631 - student_loss: 0.1148 - distillation_loss: 0.0091\n",
      "Epoch 13/30\n",
      "1875/1875 [==============================] - 3s 2ms/step - sparse_categorical_accuracy: 0.9643 - student_loss: 0.1121 - distillation_loss: 0.0088\n",
      "Epoch 14/30\n",
      "1875/1875 [==============================] - 3s 2ms/step - sparse_categorical_accuracy: 0.9649 - student_loss: 0.1081 - distillation_loss: 0.0086\n",
      "Epoch 15/30\n",
      "1875/1875 [==============================] - 3s 2ms/step - sparse_categorical_accuracy: 0.9658 - student_loss: 0.1058 - distillation_loss: 0.0084\n",
      "Epoch 16/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - sparse_categorical_accuracy: 0.9672 - student_loss: 0.1034 - distillation_loss: 0.0082\n",
      "Epoch 17/30\n",
      "1875/1875 [==============================] - 3s 2ms/step - sparse_categorical_accuracy: 0.9676 - student_loss: 0.1006 - distillation_loss: 0.0080\n",
      "Epoch 18/30\n",
      "1875/1875 [==============================] - 3s 2ms/step - sparse_categorical_accuracy: 0.9683 - student_loss: 0.0987 - distillation_loss: 0.0079\n",
      "Epoch 19/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - sparse_categorical_accuracy: 0.9690 - student_loss: 0.0971 - distillation_loss: 0.0078\n",
      "Epoch 20/30\n",
      "1875/1875 [==============================] - 3s 2ms/step - sparse_categorical_accuracy: 0.9692 - student_loss: 0.0955 - distillation_loss: 0.0077\n",
      "Epoch 21/30\n",
      "1875/1875 [==============================] - 3s 2ms/step - sparse_categorical_accuracy: 0.9703 - student_loss: 0.0929 - distillation_loss: 0.0076\n",
      "Epoch 22/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - sparse_categorical_accuracy: 0.9698 - student_loss: 0.0923 - distillation_loss: 0.0075\n",
      "Epoch 23/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - sparse_categorical_accuracy: 0.9703 - student_loss: 0.0906 - distillation_loss: 0.0074\n",
      "Epoch 24/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - sparse_categorical_accuracy: 0.9719 - student_loss: 0.0889 - distillation_loss: 0.0073\n",
      "Epoch 25/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - sparse_categorical_accuracy: 0.9714 - student_loss: 0.0880 - distillation_loss: 0.0073\n",
      "Epoch 26/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - sparse_categorical_accuracy: 0.9725 - student_loss: 0.0868 - distillation_loss: 0.0072\n",
      "Epoch 27/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - sparse_categorical_accuracy: 0.9725 - student_loss: 0.0857 - distillation_loss: 0.0072\n",
      "Epoch 28/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - sparse_categorical_accuracy: 0.9730 - student_loss: 0.0843 - distillation_loss: 0.0071\n",
      "Epoch 29/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - sparse_categorical_accuracy: 0.9736 - student_loss: 0.0830 - distillation_loss: 0.0071\n",
      "Epoch 30/30\n",
      "1875/1875 [==============================] - 4s 2ms/step - sparse_categorical_accuracy: 0.9727 - student_loss: 0.0829 - distillation_loss: 0.0071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fca0b41a670>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Distill teacher to student\n",
    "We have already trained the teacher model, and we only need to initialize a\n",
    "`Distiller(student, teacher)` instance, `compile()` it with the desired losses,\n",
    "hyperparameters and optimizer, and distill the teacher to the student.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize and compile distiller\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "distiller.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    student_loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    distillation_loss_fn=keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10,\n",
    ")\n",
    "\n",
    "# Distill teacher to student\n",
    "distiller.fit(x_train, y_train, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - sparse_categorical_accuracy: 0.9691 - student_loss: 0.1025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9690999984741211"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate student on test dataset\n",
    "distiller.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.save(\"mnist8x8_100_80_60_40_20_10-mirror.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "import onnx\n",
    "import keras2onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 22 -> 17\n",
      "The maximum opset needed by this model is only 9.\n"
     ]
    }
   ],
   "source": [
    "model = load_model('mnist8x8_100_80_60_40_20_10.h5')\n",
    "onnx_model = keras2onnx.convert_keras(model, model.name)\n",
    "onnx.save_model(onnx_model, \"mnist8x8_100_80_60_40_20_10.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tf executing eager_mode: True\n",
      "tf.keras model eager_mode: False\n",
      "The ONNX operator number change on the optimization: 13 -> 8\n",
      "The maximum opset needed by this model is only 9.\n"
     ]
    }
   ],
   "source": [
    "model = load_model('mnist8x8_100_80_60_40_20_10-mirror.h5')\n",
    "onnx_model = keras2onnx.convert_keras(model, model.name)\n",
    "onnx.save_model(onnx_model, \"mnist8x8_100_80_60_40_20_10-mirror.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The student network was trained for a total of 18 epochs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "True\n",
      "[[-440.28275   216.44778   248.75108   407.42728  -185.77899    14.428975\n",
      "  -505.653     117.48717    23.33815  -311.39465 ]]\n",
      "[[-146.83609  -150.80287    94.99972   243.09435  -368.1465   -213.94551\n",
      "  -328.0137    -70.71278   -53.830566 -466.51904 ]]\n"
     ]
    }
   ],
   "source": [
    "#point = [0.0, 0.01, 8.005, 14.005, 14.995, 13.995, 7.005, 0.01, 0.0, 0.0, 6.995, 7.995, 8.995, 14.995, 8.005, 0.0, 0.01, 0.0, 0.0, 0.01, 5.005, 13.995, 2.995, 0.0, 0.0, 0.0, 5.005, 9.005, 13.995, 13.005, 4.005, 0.0, 0.0, 1.005, 10.005, 14.005, 14.995, 12.005, 4.005, 0.01, 0.0, 0.0, 2.005, 13.005, 8.005, 1.995, 0.0, 0.01, 0.0, 0.0, 5.995, 14.995, 2.005, 0.01, 0.0, 0.01, 0.01, 0.0, 9.995, 9.995, 0.01, 0.01, 0.01, 0.01]\n",
    "#point = [0.0, 0.01, 8.005, 14.005, 14.995, 13.995, 7.005, 0.01, 0.0, 0.0, 6.995, 7.995, 8.995, 14.995, 8.005, 0.0, 0.01, 0.0, 0.0, 0.01, 5.005, 13.995, 2.995, 0.0, 0.0, 0.0, 5.005, 9.005, 13.995, 13.005, 4.005, 0.0, 0.0, 1.005, 10.005, 14.005, 14.995, 12.005, 4.005, 0.01, 0.0, 0.0, 2.005, 13.005, 8.005, 1.995, 0.0, 0.01, 0.0, 0.0, 5.995, 14.995, 2.005, 0.01, 0.0, 0.01, 0.01, 0.0, 9.995, 9.995, 0.01, 0.01, 0.01, 0.01]\n",
    "#point = [0.0, 0.01, 8.005, 14.005, 14.995, 13.995, 7.005, 0.01, 0.0, 0.0, 6.995, 7.995, 8.995, 14.995, 8.005, 0.0, 0.01, 0.0, 0.0, 0.01, 5.005, 13.995, 2.995, 0.0, 0.0, 0.0, 5.005, 9.005, 13.995, 13.005, 4.005, 0.0, 0.0, 1.005, 10.005, 14.005, 14.995, 12.005, 4.005, 0.01, 0.0, 0.0, 2.005, 13.005, 8.005, 1.995, 0.0, 0.01, 0.0, 0.0, 5.995, 14.995, 2.005, 0.01, 0.0, 0.01, 0.01, 0.0, 9.995, 9.995, 0.01, 0.01, 0.01, 0.01]\n",
    "point = [0.1, 0.1, 7.95, 14.05, 14.95, 14.05, 6.95, 0.1, 0.0, 0.1, 6.95, 8.05, 9.05, 14.95, 8.05, 0.1, 0.1, 0.0, 0.1, 0.1, 5.05, 14.05, 2.95, 0.0, 0.1, 0.0, 4.95, 9.05, 14.05, 12.95, 4.05, 0.0, 0.0, 1.05, 9.95, 14.05, 14.95, 12.05, 4.05, 0.1, 0.1, 0.1, 2.05, 13.05, 8.05, 2.05, 0.0, 0.1, 0.0, 0.0, 6.05, 15.05, 2.05, 0.0, 0.1, 0.1, 0.1, 0.1, 9.95, 10.05, 0.1, 0.1, 0.1, 0.0]\n",
    "teacher1 = load_model('mnist8x8_100_80_60_40_20_10.h5')\n",
    "student1 = load_model('mnist8x8_100_80_60_40_20_10-mirror.h5')\n",
    "p1=teacher1.predict(np.array([point]))\n",
    "p2=student1.predict(np.array([point]))\n",
    "print(np.argmax(p1)==np.argmax(p2))\n",
    "print(p1)\n",
    "print(p2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equivalence Properties\n",
    "It seems that the `9200.e` properties with `e<5` are top 1 equivalent.\n",
    "`9000*` and `9100*` on the other hand are not top 1 equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnequiv",
   "language": "python",
   "name": "nnequiv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
